
Rather than simply reiterate each chapter that has come before, in this concluding chapter I will instead try to simply state a set of basic principles and lessons that have been learned by the community and that we have encountered in the previous chapters that cover benchmarking quantum annealers in application settings. Before we do that, however, I wish to introduce one last, short, review of the progress that has been made in the field of benchmarking quantum annealers, by those other than myself.

\section{Recent progress}
An interesting critique of the scaling results presented in chapter \ref{ch:speedup} was made in Ref.~\cite{2014Katzgraber}, which argued that random Ising instances restricted to the Chimera graph are ``too easy", essentially since their phase space exhibits only a zero-temperature transition. This would imply that classical thermal algorithms such as SA see a simple energy landscape with a single global minimum throughout the entire anneal (except perhaps at the very end as the simulation temperature is lowered to near zero), instead of the usual glassy landscape with many local traps associated with hard optimization problems. This work highlighted the importance of a careful design of benchmark problems, to ensure that classical solvers would not find them trivial. Of course, it should be stressed that quantum speedup is always relative, and it can be observed even when efficient (polynomial-time) classical algorithms exist, as in, e.g., the solution of linear systems of equations \cite{PhysRevLett.103.150502}. In light of this one may interpret the message of Ref.~\cite{2014Katzgraber} to mean that a quantum speedup might not be \emph{detectable} over a finite range of problem sizes if the problem is classically easy, since the difference between the quantum and classical scaling is too subtle to be statistically significant.

Before we turn to a discussion of the evidence for a limited quantum speedup, we first briefly discuss alternatives to the TTS as a performance measure. One such alternative is the time-to-target (TTT), i.e., the total time required by a solver
to reach the target energy at least once with a desired probability, assuming each run takes a fixed time \cite{King:2015cs}. This reduces to the TTS if the target is the ground state. A unified approach that includes a variety of other measures was presented in Ref.~\cite{Vinci:2016tg}, drawing upon optimal stopping theory, specifically the so-called ``house-selling" problem \cite{Ferguson:book}. Within this framework one answers the question of how long, \emph{given a particular cost for each sample drawn from a solver}, one should sample in order to maximize one's reward, analogously to the decision problem about when to sell one's house given that bids accrue over time but that waiting longer carries a higher monetary cost. This allows the TTS and TTT, among other measures, to be shown to be specific choices of the cost and reward functions. The optimal stopping framework also paves the way for a more detailed comparison between quantum and classical approaches and the tradeoffs of each, as by altering the cost per sample one can see the impact of the distribution over states (rather than just the ground state) for the various solvers. Optimal stopping is appropriate for applications where finding the minimum energy is not strictly the most important consideration for the application, such as many machine learning contexts and even various business-origin optimization problems. In those cases, there is a tradeoff between the cost to perform the computation and the benefit from receiving a result. Tests were performed demonstrating the optimal stopping approach with a DW2X device (with $1098$ qubits) on frustrated loop problems much like those in Ref.~\cite{Hen:2015rt}, demonstrating identical scaling (modulo concerns about the lack of an optimal annealing time) to the HFS algorithm at multiple values of the cost to draw a sample, an improvement over the DW2. However, these results could still not qualify as a limited quantum speedup due to the problem of suboptimal annealing times.

This problem was finally overcome in Ref.~\cite{Albash:2017aa}, which for the first time demonstrates an optimal annealing time, and can thus make positive claims about limited quantum speedup. Previous studies could not find an optimal annealing time since a class of problem instances had not been identified for which the shortest available annealing time ($20\mu s$ in the DW2, $5\mu s$ in all other D-Wave devices) was sufficiently short to observe on optimum given the largest problem size that could be tested. Using the D-Wave 2000Q (DW2KQ) device (with $2027$ qubits) Ref.~\cite{Albash:2017aa} demonstrated a simple one-unit cell gadget Hamiltonian
which, when added randomly to a constant fraction of the unit cells on top of similar frustrated loop problems as in Ref.~\cite{DW2000Q}, resulted in the observation of an optimal annealing time for frustrated loops defined on the hardware graph (also when using the DW2X device), as well as for frustrated loops defined on the logical graph of unit cells (each unit cell then being bound together tightly as a pseudo-spin in the physical problem, modulo the gadget Hamiltonian). For the latter, logical-planted instances, the DW2KQ exhibited a statistically significant scaling advantage over both single-spin-flip SA and SVMC. These results amount to the first observation of a  limited quantum speedup, since the existence of an optimal annealing time was certified. However, this did not amount to an unqualified quantum speedup since the DW2KQ's scaling was worse than the HFS algorithm, unit-cell cluster-flip SA, and SQA, which was found to have the best scaling. Nevertheless, this result paves the way towards future demonstrations of problems with optimal annealing times and hence certifiable scaling, a necessary requirement for any type of scaling speedup claim. However, even this may not be sufficient since other quantities remain that must eventually be optimized, such as the annealing schedule, which is known to play a crucial role in provable quantum speedups (specifically the Grover search problem \cite{Roland:2002ul,RPL:10}), and can conversely be used to potentially overturn (limited) quantum speedup claims.

This provides a reasonable update of the status of benchmarking these systems, and is included here for completeness.

\section{Guidelines for benchmarking quantum annealing and related noisy quantum computational devices}

Now, finally, let us turn to a statement of principles, of sorts, for benchmarking quantum annealers.

\subsubsection{Resource use comparisons}

	It is vitally important to carefully account for resource use, lest one be led astray with a fake speedup. As was discussed in the chapters \ref{ch:speedup} and \ref{ch:planted}, classical resources should, in general, scale at least linearly with the system size of one's quantum computer. This is especially true for annealing type devices, as we have seen that annealing type algorithms are often their biggest competitors and those algorithms are typically very parallelizable. While this can be expected to ward off only linear or near-linear polynomial advantages for annealers, as was seen in chapter \ref{ch:planted}, this can be decisive, particularly in the case of non-asymptotic problem sizes.

	\subsubsection{Parameter optimization}

	It is also necessary to optimize the parameters of all solvers as best one can to be able to make any potential claims about speedup or advantage of one over another. In particular, quantum annealing requires a demonstration of an optimal annealing time for a fixed schedule before any definitive conclusion can be drawn about a quantum speedup. We saw this in chapter \ref{ch:speedup} empirically, and there is further proof (with a proof) of this in chapter \ref{ch:planted}. More generally, optimizing all known free parameters is almost certainly necessary to demonstrate a quantum speedup which will hold up to scrutiny. Ultimately, if there are free parameters of your various algorithms that have not been optimized, one is not able to make any hard claims about performance. Simulated annealing with an arbitrarily chosen number of sweeps is obviously going to fail, in general. Parallel tempering with a poorly chosen temperature spacing will also fail. In classification contexts, using suboptimal hyperparemeters of the learning algorithm will yield poor results.

	While it is true that for many algorithms one is almost always simply unable to properly optimize all the free parameters due to their enormous number (parallel tempering is an example), one should still choose best-in-class and/or standard methods for selecting said parameters. If a practioner is unfamiliar with such methods, as was the case for my work on the Higgs problem in chapter \ref{ch:higgs} for XGBoost and DNNs, one should both familiarize oneself as well as consult subject matter/application area experts (as was done in that case). In general, if one leaves free parameters unoptimized or only partially optimized, one can only make lower-bound claims on whatever performance metrics are of interest. This leads to...

\subsubsection{Distinguish between types of quantum speedup and take care in algorithm choice}

	 One must distinguish between different types of quantum speedup. Comparisons between a quantum computational device and a single other solver are inherently limited to a demonstration of a ``potential quantum speedup". In general, studies of this kind (comparing against only single solvers) have little value to the broader community, as it is simply far too easy to select a ``broken'' algorithm that is clearly suboptimal for the task at hand. An example is using single-spin flip SA in problems with clusters, as was done in \cite{PhysRevX.6.031015}, though there have been any number of similar such experiments. The only exception to this is if one is comparing against an algorithm already considered to be best-in-class (for Chimera-structured problems, that would likely be the HFS algorithm, or perhaps PT with isoenergetic cluster moves), and even then one should still, for the community's sake, study other potentially competing algorithms.

	 To go further, one must be sure to compare performance against a suite of algorithms, in particular those that mimic the device to some degree (such as SA or SQA). A speedup against such solvers would be considered a ``limited quantum speedup". (Note: This was also done in \cite{PhysRevX.6.031015}, as they did compare against PIMC, and did not find any evidence of speedup.) PIMC or some other variant of quantum Monte Carlo is vital in cases of potential quantum speedup, as it has been found to often correlate well with quantum annealers (see references \cite{q108,speedup} for instance). Finally, if there is a consensus about the solvers that are the best at the original algorithmic task, then a speedup against such solvers would be considered an unqualified ``quantum speedup". This would be a game-changing result, but as yet has never been discovered in this space.

\subsubsection{Analyze full pipelines}

	As was done in chapter \ref{ch:higgs}, one should compare one's quantum algorithm not only with classical alternatives to one's calls to the quantum device itself, but also to the best available classical methods of solving the problem. In that chapter, had we merely introduced QAML and compared DW to SA as a way of solving/sampling from the corresponding Ising Hamiltonian, the study would be been entirely pointless --- we would have had no way of knowing if our performance was good or bad, as we would not have had anything to compare QAML, itself, to. Similarly, had we merely compared raw QAML with DW as a solver with DNNs and XGBoost, we would have been unable to make any statements about whether it was the quantum annealer, or merely the QAML algorithm, that produced our performance results. This is a general problem -- merely mapping a problem into an Ising model and solving it is insufficient, unless one both tests replacements for the quantum device and algorithms for the original problem class (whether the class be solving Ising models \cite{speedup}, solving SAT problems\cite{MAX2SAT}, graph coloring\cite{perdomo2017readiness}, job shop scheduling\cite{perdomo2017readiness}, etc.). In general, this is another case where consultation between our community and subject matter/application area experts is key for rigorous and useful benchmarking studies.

\subsubsection{Gauge-averaging}

	Users of such quantum computational devices should perform something akin to gauge averaging in order to effectively estimate performance, by averaging over many different mappings from the logical problem to physical states, at least so long as the devices are not fully error corrected. Nuisance parameters, local biases, Hamiltonian dependent interactions, etc. are all expected to continue to be an issue long into the future, in the absence of error correction, and as long as that is true, the use of gauges (ie sampling over efficiently applicable maps from the logical problem to physical states) is key. Given that there is typically no good distribution for problem hardness as a function of this ensemble of mappings, nonparameteric techniques are appropriate, as was described in detail in \ref{ch:benchmarking}. This leads to...

\subsubsection{Take your state of knowledge seriously}

	As was discussed in \ref{ch:benchmarking}, and again to a degree in \ref{ch:higgs}, practioners should think carefully about what they want to learn from their experiment (time to solution, probability of success, order parameters of TTS over an instance set, classifier performance, etc.) and how the results of the experiment effect their knowledge. As stated above, for things like estimating TTS, the focus of chapter \ref{ch:benchmarking}, a theoretically well-founded and simple nonparametric approach is the Bayesian bootstrap over gauges, which may be readily extended to estimates of order parameters of TTS over instances. In other cases, something like the reweighting of our very large event test set with Poisson weights is more appropriate as it simulates the physical process which generates the statistical error. Moreover, as stated in \ref{ch:benchmarking}, by taking one's state of knowledge seriously, one can then readily employ optional stopping with a terminating condition on one's posterior density coefficient of variation to, at times, save enormous amounts of resources (as demonstrated in the aforementioned chapters simulation studies, an order of magnitude or more). I'll also refer the reader to the work by Vinci \& Lidar \cite{Vinci:2016tg} on optimal stopping for cases where one can define meaningful cost and reward functions (typically, I expect this will be limited to real-world business use cases).

\subsubsection{Choice of benchmark problem and the meaning of ``success''}

	Finally, choice of benchmark problem is key, and should be made with an eye toward the day when classical machines are vastly outpaced by quantum devices. For example, the transition from random Ising problems to frustrated loop/planted solutions problems seen between studies presented in chapters \ref{ch:speedup} and \ref{ch:planted} was forced by the need to have reasonable benchmarks for devices so large that classical systems cannot solve them in a human lifetime. If no analogue to planted solutions is feasible in the area of interest, the previous sections' exhortations about testing a wide array of solvers, optimizing parameters, and consulting with application area experts become all the more important, as one will be forced to resort to the use of ``the best solution found'' as one's definition of a ``success'', and one doesn't want to have missed an obvious or readily available algorithm that could find better solutions than any solvers one tests.
\newline\newline\newline\newline

If the reader adheres to these principles and applies them well in their use cases, and these become standard/common knowledge within the community, I believe it will significantly improve the quality, realiability, and usefulness of future benchmarking studies and, in doing so, accelerate progress in the field.
