The interest in quantum computing originates in the potential of a quantum computer to solve certain computational problems much faster than is possible classically. Examples are the factoring of integers \cite{Shor:94} or the simulation of quantum systems \cite{Feynman1982,Lloyd:1996fk}. In these examples, a quantum algorithm is exponentially faster than the best known classical algorithm. According to the extended Church-Turing thesis, all classical computers are equivalent up to polynomial factors \cite{Parberry:1986uo}. Similarly, all proposed models of quantum computation are polynomially equivalent, so that a finding of exponential quantum speedup will be model-independent. In other cases, in particular on small devices, or when the quantum speedup is polynomial, defining and detecting quantum speedup becomes more subtle. 

Denoting the time used by a specific classical device or algorithm to solve a problem of size $N$ by $C(N)$ and the time used on the quantum device by $Q(N)$, we  define quantum speedup as the asymptotic behavior of the ratio
\begin{equation}
S(N)=\frac{C(N)}{Q(N)}
\end{equation}
for $N\rightarrow\infty$. Subtleties appear in the choice of classical algorithms, in defining $C(N)$ and $Q(N)$ if the runtime depends not just on the size $N$ of a problem but also on the specific problem instance, and in extrapolating to the asymptotic limit. 

Depending on our knowledge of classical algorithms for a given problem we may consider four different types of quantum speedup. The optimal scenario is one of a ``provable quantum speedup," where there exists a proof that no classical algorithm can outperform a given quantum algorithm. The best known example is Grover's search algorithm \cite{Grover:97a}, 
which, in the query complexity setting, exhibits a provable quadratic speedup over the best possible classical algorithm \cite{Bennett:1997lh}.
%which exhibits a provable quadratic speedup over the best possible classical algorithm \cite{Bennett:1997lh}, assuming an oracle. 
A ``strong quantum speedup" was defined in \cite{Traub2013} by using the performance of the {best} classical algorithm for $C(N)$, whether such an algorithm is known or not. Unfortunately, the performance of the best classical algorithm is unknown for many interesting problems. In the case of factoring, for example, a proof of a classical super-polynomial lower-bound is not known. A less ambitious goal is therefore desirable, and thus one usually defines ``quantum speedup" (without additional adjectives) by comparing to the best available classical algorithm instead of the best possible classical algorithm.  

 A weaker scenario is one where a quantum algorithm is designed to make use of quantum effects, but it is not known whether these quantum effects provide an advantage over classical algorithms or where a device is a putative or candidate quantum information processor. To capture this scenario, which is of central interest to us in this work, we define ``limited quantum speedup" as a speedup obtained when comparing specifically with classical algorithms that `correspond' to the quantum algorithm in the sense that they implement the same algorithmic approach, but on classical hardware. A natural example is quantum annealing \cite{Kadowaki1998,Brooke1999} or adiabatic quantum optimization \cite{farhi} implemented on a candidate physical quantum information processor \textit{vs} corresponding classical algorithms such as simulated annealing (SA) \cite{Kirkpatrick1983} (which performs annealing on a classical Monte Carlo simulation of the Ising spin glass and makes no use of quantum effects) or simulated quantum annealing (SQA) \cite{PhysRevB.66.094203,Santoro} (a classical algorithm mimicking quantum annealing in a path-integral quantum Monte Carlo simulation). In this comparison a limited quantum speedup would be a demonstration that quantum effects improve the annealing algorithm.


%\section{Classical and quantum annealing of a spin glass}

To illustrate the subtleties in detecting quantum speedup, even after a classical reference algorithm is chosen, we will compare the performance of an experimental $503$-qubit D-Wave Two (DW2) device to classical algorithms and analyze the evidence for quantum speedup on the benchmark problem of random spin glass instances. Specifically, we consider the problem of finding the ground state of an Ising spin glass model described by a  ``problem Hamiltonian''  
\begin{equation}
H_{\mathrm{Ising}} = -\sum_{i\in \mc{V}} h_i \sigma_i^z - \sum_{(i,j)\in \mc{E}} J_{ij} \sigma_i^z \sigma_j^z \ ,
\label{eq:H}
\end{equation}
with $N$ binary variables $\sigma_i^z = \pm 1$. The local fields $\{h_i\}$ and couplings $\{J_{ij}\}$ are fixed and define a {problem instance} of the Ising model.
 The spins occupy the vertices $\mc{V}$ of a graph $G=\{\mc{V},\mc{E}\}$ with edge set $\mc{E}$. We will consider the distributions of the time to solution over many random spin glass problems  instances with zero local fields on the `Chimera graph' realized by the DW2 device (see Supplementary Material \cite{SM} for a definition of that graph and why we choose $h_i=0$). This problem is NP-hard\cite{Barahona1982} and all known classical algorithms scale super-polynomially not only for the hardest but also for typical instances. While quantum mechanics is not expected to reduce the super-polynomial scaling to polynomial, a quantum algorithm might still scale better with problem size $N$ than any classical algorithm. 

The D-Wave devices \cite{Harris2010,0953-2048-23-6-065004,berkley2010scalable,Johnson2011} are designed to be physical realizations of quantum annealing using superconducting flux qubits and programmable fields $\{h_i\}$ and couplings $\{J_{ij}\}$. Quantum annealing is implemented by initializing the system in the ground state of a transverse magnetic field $H_X = -\sum_{i\in\mathcal{V}} \sigma_i^x$, where the $\sigma$'s denote the Pauli matrices; $H_X$ is turned off adiabatically while $H_{\mathrm{Ising}}$ is turned on simultaneously. A measurement of each qubit in the computational basis then determines the ground state of $H_{\mathrm{Ising}}$ with a probability affected by many factors, including thermal excitations and implementation errors \cite{ourpaper}. Tests on small problems on D-Wave One (DW1) \cite{Boixo2012} and DW2 devices \cite{Vinci:2014yq} have shown that for small problem sizes the device correlates well with the predictions of a quantum master equation. For larger problem sizes a $108$ qubit DW1 device correlated well with simulated quantum annealing (SQA) \cite{ourpaper}, which indicates that despite decoherence and coupling to a thermal bath, the behavior of the device is consistent with it actually performing quantum annealing \cite{Smolin,comment-SS}. However, for these benchmarks both SQA and the DW1 device are also well described by a semi-classical mean-field version of SQA \cite{SSSV}, which raises the question whether quantum effects play an important role. The approach adopted here, of seeking evidence of a (limited) quantum speedup, directly addresses the crucial question of whether large-scale quantum effects create a potential for the devices to outperform classical algorithms. To test this possibility we compare the performance of a DW2 device to two `corresponding' classical algorithms: SA and SQA.

%\section{Considerations when computing quantum speedup}

Since quantum speedup concerns the asymptotic scaling of $S(N)$ we consider the subtleties of estimating it from small problem sizes $N$, and inefficiencies at small problem sizes that can fake or mask a speedup. In the context of annealing, the optimal choice of the annealing time $t_a$ turns out to be crucial for estimating asymptotic scaling. To illustrate this we first consider the time to solution using SA and SQA run at different {fixed} annealing times $t_a$, independent of the problem size $N$. Figure~\ref{fig:medianfixed}A shows the scaling of the median total annealing time (over $1000$ different random instances on the D-Wave Chimera graph -- see Supplementary Material \cite{SM}) for SQA to find a solution at least once with probability $p=0.99$. Corresponding times for SA are shown in figure S10. We observe that at constant $t_a$, as long as $t_a$ is long enough to find the ground state almost every time, the scaling of the total effort is at first relatively flat. The total effort then rises more rapidly, once one reaches problem sizes for which the chosen annealing time is too short, and the success probabilities are thus low, requiring many repetitions. Extrapolations to $N\rightarrow\infty$ need to consider the lower envelope of all curves, which corresponds to choosing an optimal annealing time $t_a^{\rm opt}(N)$ for each $N$.

Figure~\ref{fig:medianfixed}B demonstrates that when using fixed annealing times no conclusion can be drawn from annealing (simulated or in a device) about the asymptotic scaling . The initial slow increase at constant $t_a$ is misleading and instead the optimal annealing time $t_a^{\rm opt}$ needs to be used for each problem size $N$. To illustrate this we show in Figure~\ref{fig:medianfixed}B the real ``speedup'' ratio of the scaling of SA and SQA (actually a slowdown), and a  fake speedup due to a constant and excessively long annealing time $t_a$ for SQA. Since SA outperforms SQA on our benchmark set, it is our algorithm of choice in the comparisons with the DW2 reported below.

%\subsection{Resource usage and speedup from parallelism}

A related issue is the scaling of hardware resources (computational gates and memory) with problem size, which must be identical for the devices we compare. A  device whose hardware resources scale as $N$ can achieve an intrinsic parallel speedup compared to a fixed size device. Such is the case for the DW2, which uses $N$ (out of 512) qubits and $\mc{O}(N)$ couplers and classical logical control gates to solve a spin glass instance with $N$ spin variables in time $T_{\textrm{DW}}(N)$. Considering quantum speedup for $N\rightarrow\infty$ we need to compare a (hypothetical) larger DW2 device with the number of qubits and couplers growing as $\mc{O}(N)$  to a (hypothetical) classical device with $\mc{O}(N)$ gates or processing units. Since SA (and SQA) are perfectly parallelizable for the bipartite Chimera graphs realized by the DW2, we can relate the scaling of the  time $T_{\rm C}(N)$ on such a device to the time $T_{\rm SA}(N)$ for SA on a fixed size classical CPU by $T_C(N)\sim T_{\rm SA}/N$ and obtain
\begin{equation}
S(N) 
= \frac{T_{\textrm{C}}(N)}{T_{\textrm{DW}}(N)} 
\sim  \frac{T_{\textrm{SA}}(N)}{T_{\textrm{DW}}(N)} \frac{1}{N}.
\label{eq:parallelspeedup1}
\end{equation}
%

%\section{Performance of  D-Wave Two versus SA and SQA}

We finally address the question of how to measure time when the time to solution depends on the specific problem instance. When a device is used as a tool for solving computational problems, the question of interest is to determine which device is better for almost all possible problem instances. If instead the focus is on the underlying physics of a device then it might suffice to find a subclass of instances where a speedup is exhibited. These two questions lead to different quantities of interest. 

%\subsection{Performance as an optimizer: comparing the scaling of hard problem instances}

To illustrate the considerations we now turn to our results. Figure~\ref{fig:scalingraw} shows the scaling of the time to find the ground state for various quantiles, from the easiest instances (1\%) to the hardest (99\%), comparing the DW2 and SA. We chose the values of the couplings $J_{ij}$ from $2r$ discrete values  $\{n/r\}$, with $n \in \pm\{1, \dots, r-1, r\}$, and call $r$ the ``range''. Since we do not \textit{a priori} know the hardness of a given problem instance we have to assume the worst case and perform a sufficient number of repetitions $R$ to be able to solve even the hardest problem instances. Hence the scaling for the highest quantiles is the most informative. Here, we consider only the pure annealing times, ignoring setup and readout times that scale subdominantly (see \cite{SM} for wall-clock results). 

We observe for both the DW2 and SA, for sufficiently large $N$, that the total time to solution for each quantile $q$ scales as $\exp(c_q\sqrt{N})$ (with $c_q>0$ a constant), as reported previously for SA and SQA \cite{ourpaper}. The origin of the $\sqrt{N}$ exponent is well understood for exact solvers as reflecting the treewidth of the Chimera graph \cite{SM,Choi2}. While the SA code was run at an optimized annealing time for each problem size $N$, the DW2 has a minimal annealing time of $t_a=20\mu s$, which is longer than the optimal time for all problem sizes \cite{SM}. Therefore the observed slope of the DW2 data can only be taken as a lower bound for the asymptotic scaling. With this in mind, we observe similar scaling for SA and the DW2 for $N\gtrsim 200$.

How can we probe for a speedup in light of this similar scaling? With algorithms such as SA or quantum annealing, where the time to solution depends on the problem instance, it is impractical to experimentally find the hardest problem instance. If instead we target a fraction of $q$\% of the instances then we should consider the $q$th quantile in the scaling plots shown in Figure~\ref{fig:scalingraw}. The appropriate speedup quantity is then the ratio of these quantiles (``RofQ''). Denoting a quantile $q$ of a random variable $X$ by $[X]_q$ we define this as
\begin{equation}
S^{\textrm{RofQ}}_q(N) = 
%\frac{[T_{\textrm{C}}(N)]_q}{[T_{\textrm{DW}}(N)]_q} \propto 
\frac{[T_{\textrm{SA}}(N)]_q}{[T_{\textrm{DW}}(N)]_q}  \frac{1}{N} ,
\label{eq:S_q}
\end{equation}
Plotting this quantity for the DW2 \textit{vs} SA in Figure~\ref{fig:qorspeedup7} (A and B) we find no evidence for a limited quantum speedup in the interesting regime of large $N$ and large $q$ (almost all instances). That is, while for all quantiles and for both ranges the initial slope is positive, when $N$ and $q$ become large enough we observe a turnaround and eventually a negative slope. While we observe a positive slope for quantiles smaller than the median, this is of limited interest since we have not been able to identify a priori which instances will be easy. Taking into account that due to the fixed suboptimal annealing times the speedup defined in Eq.~\eqref{eq:S_q} is an upper bound, we conclude that there is no evidence of a speedup over SA for this particular benchmark.

%\subsection{Instance-by-instance comparison}
%\label{sec:qor}

$S^{\textrm{RofQ}}_q(N)$ measures the speedup while comparing different sets of instances for DW and SA, each determined by the respective quantile. Now we consider instead whether there is a speedup for a (potentially small) subset of the same problem instances. To this end we study the scaling of the ratios of the time to solution for individual instances, and display in Figure~\ref{fig:qorspeedup7} (C and D) the scaling of various quantiles of the ratio (``QofR")
\begin{equation}
S_q^{\textrm{QofR}}(N) = 
%\left[\frac{T_{\textrm{C}}(N)}{T_{\textrm{DW}}(N)}\right]_q \propto 
\left[\frac{T_{\textrm{SA}}(N)}{T_{\textrm{DW}}(N)}\right]_q \frac{1}{N} .
\label{eq:SQoR}
\end{equation}
For $r=7$ all the quantiles bend down for sufficiently large $N$, so that there is no evidence of a limited quantum speedup. There does seem to be an indication of such a speedup compared to SA in the low quantiles for $r=1$, i.e., for those instances whose speedup ratio was high. However, the instances contributing here are not run at the optimal annealing time, and more work is needed to establish that the potential $r=1$ speedup result persists for those instances for which one can be sure that the annealing time is optimal. 

Next we consider the distribution of solution times at a fixed problem size. This does not address the speedup question since no scaling can be extracted, but illuminates instead the question of correlation between the performance of the DW2 and SA. To this end we perform individual comparisons for each instance and show in Figure~\ref{fig:ratiosannealing}A-B the time to solution for the same instances for the DW2 and SA. We observe a wide scatter (in agreement with the DW1 results of Ref.~\cite{ourpaper}) and find that while the DW2 is sometimes up to $10\times$ faster in pure annealing time, there are many cases where it is $\geq 100\times$ slower. 

%\section{Discussion}

It is not yet known whether a quantum annealer or even a perfectly coherent adiabatic quantum optimizer can exhibit (limited) quantum speedup at all, although there are promising indications from theory~\cite{Somma:2012kx}, simulation \cite{Santoro}, and experiments on spin glass materials \cite{Brooke1999}. Experimental tests are thus important. 
%We chose to focus here on the benchmark problem of random zero-field Ising problems parametrized by the range of couplings. We did not find compelling evidence of limited quantum speedup for the DW2 relative to SA in our particular benchmark set when we considered the ratio of quantiles of time to solution, which is the relevant quantity for the performance of a device as an optimizer. 
%When we focus on subsets of problem instances in an instance-by-instance comparison, we observe a possibility for a limited quantum speedup for a fraction of the instances. However, since the DW2 runs at a suboptimal annealing time for most of the corresponding problem instances, the observed speedup may be an artifact of attempting to solve the smaller problem sizes using  an excessively long annealing time. 
There are several candidate explanations for the absence of a clear quantum speedup in our tests. Perhaps quantum annealing simply does not provide any advantages over simulated (quantum) annealing or other classical algorithms for the problem class we have studied \cite{2014Katzgraber}; or, perhaps, the noisy implementation in the DW2 cannot realize quantum speedup and is thus not better than classical devices. Alternatively, a speedup might be masked by calibration errors, improvements might arise from error correction \cite{PAL:13}, or other problem classes might exhibit a speedup. Future studies will probe these alternatives and aim to determine whether one can find a class of problem instances for which an unambiguous speedup over classical hardware can be observed. 

While we used specific processors and algorithms for illustration, the considerations about a reliable determination of quantum speedup presented here are general. For any speedup analysis, using the same scaling of hardware resources for both quantum and classical devices is required to disentangle parallel and quantum speedup. And, for any quantum algorithm where the runtime must be determined experimentally, a careful extrapolation to large problem sizes is important to avoid mistaking inefficiencies at small problem sizes for signs of quantum speedup.

%%%%%%%%%%%%%%%%%%%%%




